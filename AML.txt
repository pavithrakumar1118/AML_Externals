1st Program
--------------------
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
# Load the Iris dataset
X, y = load_iris(return_X_y=True)
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,␣
↪random_state=42)
# Create and train a k-NN classifier with k=3
knn = KNeighborsClassifier(n_neighbors=3).fit(X_train, y_train)
# Predict the labels for the test set
y_pred = knn.predict(X_test)
# Calculate the accuracy
accuracy = accuracy_score(y_test, y_pred)
# Print the correct and wrong predictions
for actual, predicted in zip(y_test, y_pred):
result = "Correct" if actual == predicted else "Wrong"
print(f"{result} Prediction: Actual = {actual}, Predicted = {predicted}")
# Print summary
print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"Correct Predictions: {sum(y_test == y_pred)}")
print(f"Wrong Predictions: {sum(y_test != y_pred)}")


2nd Program
-----------------------------------
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt

iris = load_iris()
X = iris.data[:, [2, 1]]

n_clusters = 3

kmeans = KMeans(n_clusters=n_clusters, random_state=0)
kmeans_labels = kmeans.fit_predict(X)

gmm = GaussianMixture(n_components=n_clusters, random_state=0)
gmm_labels = gmm.fit_predict(X)

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='viridis')
plt.title('K-means Clustering')

plt.subplot(1, 2, 2)
plt.scatter(X[:, 0], X[:, 1], c=gmm_labels, cmap='viridis')
plt.title('EM Clustering')

plt.tight_layout()
plt.show()


3rd Program
-----------------------------
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(0)
X = np.sort(5 * np.random.rand(80, 1), axis=0)
y = np.sin(X).ravel() + 0.2 * np.random.randn(80)

def loess(x, X, y, tau=0.5):
    weights = np.exp(-((X - x) ** 2) / (2 * tau ** 2))
    theta = np.sum(X * weights) / np.sum(X ** 2 * weights)
    return theta * x

x_pred = np.linspace(0, 5, 100)
y_pred = [loess(x, X, y) for x in x_pred]  # Use default tau

plt.scatter(X, y, c='r', marker='x', label='Data')
plt.plot(x_pred, y_pred, c='b', label='LOESS Fit')
plt.legend()
plt.title('Locally Weighted Regression (LOESS)')
plt.show()



4th Program
-----------------------------
from tensorflow import keras

X = [[0, 0], [0, 1], [1, 0], [1, 1]]
y = [0, 1, 1, 0]

model = keras.Sequential([
    keras.layers.Dense(4, activation='relu', input_shape=(2,)),
    keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X, y, epochs=10)

loss, accuracy = model.evaluate(X, y)
print(f"Loss: {loss}, Accuracy: {accuracy}")


5th program
-------------------------------
import random

def generate_random_string(length):
    sample = '!@#$%^&*()_+=-?><,./:";{\|][}ABCDEFGHIJKLMNOPQRSTUVWXYZqwertyuiopasdfghjklzxcvbnm '
    return ''.join(random.choice(sample) for _ in range(length))

def print_hello_world():
    target_string = "HELLO WORLD"
    max_generations = 1000

    current_string = generate_random_string(len(target_string))

    for generation in range(1, max_generations + 1):
        current_string = ''.join(char if current_string[i] == char else generate_random_string(1) for i, char in enumerate(target_string))
        print(f"Generation {generation}: {current_string}")
        if current_string == target_string:
            break

# Call the function to print the iterations
print_hello_world()



6th Program
------------------------------
import numpy as np

env = np.array([[0, 0, 0, 1, 0], [0, 1, 0, 1, 0], [0, 1, 0, 1, 0], [0, 0, 0, 1, 2]])

Q = np.zeros((np.prod(env.shape), 4))
learning_rate, discount_factor, exploration_prob, num_episodes = 0.8, 0.95, 0.2, 1000
actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]

def state_to_index(state, shape):
    return state[0] * shape[1] + state[1]

def update_q(Q, state, action, new_state, reward):
    Q[state_to_index(state, env.shape)][action] += learning_rate * (
        reward + discount_factor * np.max(Q[state_to_index(new_state, env.shape)]) - Q[state_to_index(state, env.shape)][action]
    )

for _ in range(num_episodes):
    state = (0, 0)
    while env[state] != 2:
        action = np.random.choice(4) if np.random.uniform(0, 1) < exploration_prob else np.argmax(Q[state_to_index(state, env.shape)])
        new_state = tuple(np.clip(np.add(state, actions[action]), [0, 0], np.subtract(env.shape, [1, 1])))
        reward = -1 if env[new_state] == 1 else (10 if env[new_state] == 2 else 0)
        update_q(Q, state, action, new_state, reward)
        state = new_state

state, optimal_path = (0, 0), [(0, 0)]

while env[state] != 2:
    action = np.argmax(Q[state_to_index(state, env.shape)])
    state = tuple(np.clip(np.add(state, actions[action]), [0, 0], np.subtract(env.shape, [1, 1])))
    optimal_path.append(state)

print("Optimal Path: ", optimal_path)

